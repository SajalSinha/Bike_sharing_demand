{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled29.ipynb",
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1fGmFKYtPuBa2KF0-o64ez68YTsqf8jam",
      "authorship_tag": "ABX9TyOtkZpDL8TQBCvco68QQjJH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SajalSinha/Bike_sharing_demand/blob/main/Untitled29.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3_UYlusdffO"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/martingerlach/hSBM_Topicmodel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymc3"
      ],
      "metadata": {
        "id": "z9wEBbXMeKq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os,sys,argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter,defaultdict\n",
        "import pickle\n",
        "import graph_tools as gt\n",
        "\n",
        "\n",
        "class sbmtm():\n",
        "    '''\n",
        "    Class for topic-modeling with sbm's.\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.g = None ## network\n",
        "\n",
        "        self.words = [] ## list of word nodes\n",
        "        self.documents = [] ## list of document nodes\n",
        "\n",
        "        self.state = None ## inference state from graphtool\n",
        "        self.groups = {} ## results of group membership from inference\n",
        "        self.mdl = np.nan ## minimum description length of inferred state\n",
        "        self.L = np.nan ## number of levels in hierarchy\n",
        "\n",
        "    def make_graph(self,list_texts, documents = None, counts=True, n_min = None):\n",
        "        '''\n",
        "        Load a corpus and generate the word-document network\n",
        "        optional arguments:\n",
        "        - documents: list of str, titles of documents\n",
        "        - counts: save edge-multiplicity as counts (default: True)\n",
        "        - n_min, int: filter all word-nodes with less than n_min counts (default None)\n",
        "        '''\n",
        "        D = len(list_texts)\n",
        "\n",
        "        ## if there are no document titles, we assign integers 0,...,D-1\n",
        "        ## otherwise we use supplied titles\n",
        "        if documents == None:\n",
        "            list_titles = [str(h) for h in range(D)]\n",
        "        else:\n",
        "            list_titles = documents\n",
        "\n",
        "        ## make a graph\n",
        "        ## create a graph\n",
        "        g = gt.Graph(directed=False)\n",
        "        ## define node properties\n",
        "        ## name: docs - title, words - 'word'\n",
        "        ## kind: docs - 0, words - 1\n",
        "        name = g.vp[\"name\"] = g.new_vp(\"string\")\n",
        "        kind = g.vp[\"kind\"] = g.new_vp(\"int\")\n",
        "        if counts:\n",
        "            ecount = g.ep[\"count\"] = g.new_ep(\"int\")\n",
        "\n",
        "        docs_add = defaultdict(lambda: g.add_vertex())\n",
        "        words_add = defaultdict(lambda: g.add_vertex())\n",
        "\n",
        "        ## add all documents first\n",
        "        for i_d in range(D):\n",
        "            title = list_titles[i_d]\n",
        "            d=docs_add[title]\n",
        "\n",
        "        ## add all documents and words as nodes\n",
        "        ## add all tokens as links\n",
        "        for i_d in range(D):\n",
        "            title = list_titles[i_d]\n",
        "            text = list_texts[i_d]\n",
        "\n",
        "            d=docs_add[title]\n",
        "            name[d] = title\n",
        "            kind[d] = 0\n",
        "            c=Counter(text)\n",
        "            for word,count in c.items():\n",
        "                w=words_add[word]\n",
        "                name[w] = word\n",
        "                kind[w] = 1\n",
        "                if counts:\n",
        "                    e = g.add_edge(d, w)\n",
        "                    ecount[e] = count\n",
        "                else:\n",
        "                    for n in range(count):\n",
        "                        g.add_edge(d,w)\n",
        "\n",
        "        ## filter word-types with less than n_min counts\n",
        "        if n_min is not None:\n",
        "            v_n = g.new_vertex_property(\"int\")\n",
        "            for v in g.vertices():\n",
        "                v_n[v] = v.out_degree()\n",
        "\n",
        "            v_filter =  g.new_vertex_property(\"bool\")\n",
        "            for v in g.vertices():\n",
        "                if v_n[v] < n_min and g.vp['kind'][v]==1:\n",
        "                    v_filter[v] = False\n",
        "                else:\n",
        "                    v_filter[v] = True\n",
        "            g.set_vertex_filter(v_filter)\n",
        "            g.purge_vertices()\n",
        "            g.clear_filters()\n",
        "\n",
        "\n",
        "        self.g = g\n",
        "        self.words = [ g.vp['name'][v] for v in  g.vertices() if g.vp['kind'][v]==1   ]\n",
        "        self.documents = [ g.vp['name'][v] for v in  g.vertices() if g.vp['kind'][v]==0   ]\n",
        "\n",
        "    def make_graph_from_BoW_df(self, df, counts=True, n_min=None):\n",
        "        \"\"\"\n",
        "        Load a graph from a Bag of Words DataFrame\n",
        "        arguments\n",
        "        -----------\n",
        "        df should be a DataFrame with where df.index is a list of words and df.columns a list of documents\n",
        "        optional arguments:\n",
        "        - counts: save edge-multiplicity as counts (default: True)\n",
        "        - n_min, int: filter all word-nodes with less than n_min counts (default None)\n",
        "        :type df: DataFrame\n",
        "        \"\"\"\n",
        "        # make a graph\n",
        "        g = gt.Graph(directed=False)\n",
        "        ## define node properties\n",
        "        ## name: docs - title, words - 'word'\n",
        "        ## kind: docs - 0, words - 1\n",
        "        name = g.vp[\"name\"] = g.new_vp(\"string\")\n",
        "        kind = g.vp[\"kind\"] = g.new_vp(\"int\")\n",
        "        if counts:\n",
        "            ecount = g.ep[\"count\"] = g.new_ep(\"int\")\n",
        "\n",
        "        X = df.values\n",
        "\n",
        "        # add all documents and words as nodes\n",
        "        # add all tokens as links\n",
        "        X = scipy.sparse.coo_matrix(X)\n",
        "\n",
        "        if not counts and X.dtype != int:\n",
        "            X_int = X.astype(int)\n",
        "            if not np.allclose(X.data, X_int.data):\n",
        "                raise ValueError('Data must be integer if '\n",
        "                                 'weighted_edges=False')\n",
        "            X = X_int\n",
        "\n",
        "        docs_add = defaultdict(lambda: g.add_vertex())\n",
        "        words_add = defaultdict(lambda: g.add_vertex())\n",
        "\n",
        "        D = len(df.columns)\n",
        "        ## add all documents first\n",
        "        for i_d in range(D):\n",
        "            title = df.columns[i_d]\n",
        "            d = docs_add[title]\n",
        "            name[d] = title\n",
        "            kind[d] = 0\n",
        "\n",
        "        ## add all words\n",
        "        for i_d in range(len(df.index)):\n",
        "            word = df.index[i_d]\n",
        "            w = words_add[word]\n",
        "            name[w] = word\n",
        "            kind[w] = 1\n",
        "\n",
        "        ## add all documents and words as nodes\n",
        "        ## add all tokens as links\n",
        "        for i_d in range(D):\n",
        "            title = df.columns[i_d]\n",
        "            text = df[title]\n",
        "            for i_w, word, count in zip(range(len(df.index)), df.index, text):\n",
        "                if count < 1:\n",
        "                    continue\n",
        "                if counts:\n",
        "                    e = g.add_edge(i_d, D + i_w)\n",
        "                    ecount[e] = count\n",
        "                else:\n",
        "                    for n in range(count):\n",
        "                        g.add_edge(i_d, D + i_w)\n",
        "\n",
        "        ## filter word-types with less than n_min counts\n",
        "        if n_min is not None:\n",
        "            v_n = g.new_vertex_property(\"int\")\n",
        "            for v in g.vertices():\n",
        "                v_n[v] = v.out_degree()\n",
        "\n",
        "            v_filter = g.new_vertex_property(\"bool\")\n",
        "            for v in g.vertices():\n",
        "                if v_n[v] < n_min and g.vp['kind'][v] == 1:\n",
        "                    v_filter[v] = False\n",
        "                else:\n",
        "                    v_filter[v] = True\n",
        "            g.set_vertex_filter(v_filter)\n",
        "            g.purge_vertices()\n",
        "            g.clear_filters()\n",
        "\n",
        "        self.g = g\n",
        "        self.words = [g.vp['name'][v] for v in g.vertices() if g.vp['kind'][v] == 1]\n",
        "        self.documents = [g.vp['name'][v] for v in g.vertices() if g.vp['kind'][v] == 0]\n",
        "        return self\n",
        "\n",
        "    def save_graph(self,filename = 'graph.gt.gz'):\n",
        "        '''\n",
        "        Save the word-document network generated by make_graph() as filename.\n",
        "        Allows for loading the graph without calling make_graph().\n",
        "        '''\n",
        "        self.g.save(filename)\n",
        "\n",
        "    def load_graph(self,filename = 'graph.gt.gz'):\n",
        "        '''\n",
        "        Load a word-document network generated by make_graph() and saved with save_graph().\n",
        "        '''\n",
        "        self.g = gt.load_graph(filename)\n",
        "        self.words = [ self.g.vp['name'][v] for v in  self.g.vertices() if self.g.vp['kind'][v]==1   ]\n",
        "        self.documents = [ self.g.vp['name'][v] for v in  self.g.vertices() if self.g.vp['kind'][v]==0   ]\n",
        "\n",
        "\n",
        "    def fit(self,overlap = False, n_init = 1, verbose=False, epsilon=1e-3):\n",
        "        '''\n",
        "        Fit the sbm to the word-document network.\n",
        "        - overlap, bool (default: False). Overlapping or Non-overlapping groups.\n",
        "            Overlapping not implemented yet\n",
        "        - n_init, int (default:1): number of different initial conditions to run in order to avoid local minimum of MDL.\n",
        "        '''\n",
        "        g = self.g\n",
        "        if g is None:\n",
        "            print('No data to fit the SBM. Load some data first (make_graph)')\n",
        "        else:\n",
        "            if overlap and \"count\" in g.ep:\n",
        "                raise ValueError(\"When using overlapping SBMs, the graph must be constructed with 'counts=False'\")\n",
        "            clabel = g.vp['kind']\n",
        "\n",
        "            state_args = {'clabel': clabel, 'pclabel': clabel}\n",
        "            if \"count\" in g.ep:\n",
        "                state_args[\"eweight\"] = g.ep.count\n",
        "\n",
        "            ## the inference\n",
        "            mdl = np.inf ##\n",
        "            for i_n_init in range(n_init):\n",
        "                base_type = gt.BlockState if not overlap else gt.OverlapBlockState\n",
        "                state_tmp = gt.minimize_nested_blockmodel_dl(g,\n",
        "                                                             state_args=dict(\n",
        "                                                                 base_type=base_type,\n",
        "                                                                 **state_args),\n",
        "                                                             multilevel_mcmc_args=dict(\n",
        "                                                                 verbose=verbose))\n",
        "                L = 0\n",
        "                for s in state_tmp.levels:\n",
        "                    L += 1\n",
        "                    if s.get_nonempty_B() == 2:\n",
        "                        break\n",
        "                state_tmp = state_tmp.copy(bs=state_tmp.get_bs()[:L] + [np.zeros(1)])\n",
        "                # state_tmp = state_tmp.copy(sampling=True)\n",
        "                # delta = 1 + epsilon\n",
        "                # while abs(delta) > epsilon:\n",
        "                #     delta = state_tmp.multiflip_mcmc_sweep(niter=10, beta=np.inf)[0]\n",
        "                #     print(delta)\n",
        "                print(state_tmp)\n",
        "\n",
        "                mdl_tmp = state_tmp.entropy()\n",
        "                if mdl_tmp < mdl:\n",
        "                    mdl = 1.0*mdl_tmp\n",
        "                    state = state_tmp.copy()\n",
        "\n",
        "            self.state = state\n",
        "            ## minimum description length\n",
        "            self.mdl = state.entropy()\n",
        "            L = len(state.levels)\n",
        "            if L == 2:\n",
        "                self.L = 1\n",
        "            else:\n",
        "                self.L = L-2\n",
        "\n",
        "\n",
        "    def plot(self, filename = None,nedges = 1000):\n",
        "        '''\n",
        "        Plot the graph and group structure.\n",
        "        optional:\n",
        "        - filename, str; where to save the plot. if None, will not be saved\n",
        "        - nedges, int; subsample  to plot (faster, less memory)\n",
        "        '''\n",
        "        self.state.draw(layout='bipartite', output=filename,\n",
        "                        subsample_edges=nedges, hshortcuts=1, hide=0)\n",
        "\n",
        "\n",
        "    def topics(self, l=0, n=10):\n",
        "        '''\n",
        "        get the n most common words for each word-group in level l.\n",
        "        return tuples (word,P(w|tw))\n",
        "        '''\n",
        "        # dict_groups = self.groups[l]\n",
        "        dict_groups = self.get_groups(l=l)\n",
        "\n",
        "        Bw = dict_groups['Bw']\n",
        "        p_w_tw = dict_groups['p_w_tw']\n",
        "\n",
        "        words = self.words\n",
        "\n",
        "        ## loop over all word-groups\n",
        "        dict_group_words = {}\n",
        "        for tw in range(Bw):\n",
        "            p_w_ = p_w_tw[:,tw]\n",
        "            ind_w_ = np.argsort(p_w_)[::-1]\n",
        "            list_words_tw = []\n",
        "            for i in ind_w_[:n]:\n",
        "                if p_w_[i] > 0:\n",
        "                    list_words_tw+=[(words[i],p_w_[i])]\n",
        "                else:\n",
        "                    break\n",
        "            dict_group_words[tw] = list_words_tw\n",
        "        return dict_group_words\n",
        "\n",
        "    def topicdist(self, doc_index, l=0):\n",
        "        # dict_groups =  self.groups[l]\n",
        "        dict_groups = self.get_groups(l=l)\n",
        "\n",
        "        p_tw_d = dict_groups['p_tw_d']\n",
        "        list_topics_tw = []\n",
        "        for tw,p_tw in enumerate(p_tw_d[:,doc_index]):\n",
        "                list_topics_tw += [(tw,p_tw)]\n",
        "        return list_topics_tw\n",
        "\n",
        "    def clusters(self,l=0,n=10):\n",
        "        '''\n",
        "        Get n 'most common' documents from each document cluster.\n",
        "        most common refers to largest contribution in group membership vector.\n",
        "        For the non-overlapping case, each document belongs to one and only one group with prob 1.\n",
        "        '''\n",
        "        # dict_groups = self.groups[l]\n",
        "        dict_groups = self.get_groups(l=l)\n",
        "        Bd = dict_groups['Bd']\n",
        "        p_td_d = dict_groups['p_td_d']\n",
        "\n",
        "        docs = self.documents\n",
        "        ## loop over all word-groups\n",
        "        dict_group_docs = {}\n",
        "        for td in range(Bd):\n",
        "            p_d_ = p_td_d[td,:]\n",
        "            ind_d_ = np.argsort(p_d_)[::-1]\n",
        "            list_docs_td = []\n",
        "            for i in ind_d_[:n]:\n",
        "                if p_d_[i] > 0:\n",
        "                    list_docs_td+=[(docs[i],p_d_[i])]\n",
        "                else:\n",
        "                    break\n",
        "            dict_group_docs[td] = list_docs_td\n",
        "        return dict_group_docs\n",
        "\n",
        "    def clusters_query(self,doc_index,l=0):\n",
        "        '''\n",
        "        Get all documents in the same group as the query-document.\n",
        "        Note: Works only for non-overlapping model.\n",
        "        For overlapping case, we need something else.\n",
        "        '''\n",
        "        # dict_groups = self.groups[l]\n",
        "        dict_groups = self.get_groups(l=l)\n",
        "        Bd = dict_groups['Bd']\n",
        "        p_td_d = dict_groups['p_td_d']\n",
        "\n",
        "        documents = self.documents\n",
        "        ## loop over all word-groups\n",
        "        dict_group_docs = {}\n",
        "        td = np.argmax(p_td_d[:,doc_index])\n",
        "\n",
        "        list_doc_index_sel = np.where(p_td_d[td,:]==1)[0]\n",
        "\n",
        "        list_doc_query = []\n",
        "\n",
        "        for doc_index_sel in list_doc_index_sel:\n",
        "            if doc_index != doc_index_sel:\n",
        "                list_doc_query += [(doc_index_sel,documents[doc_index_sel])]\n",
        "\n",
        "        return list_doc_query\n",
        "\n",
        "\n",
        "    def group_membership(self,l=0):\n",
        "        '''\n",
        "        Return the group-membership vectors for\n",
        "            - document-nodes, p_td_d, array with shape Bd x D\n",
        "            - word-nodes, p_tw_w, array with shape Bw x V\n",
        "        It gives the probability of a nodes belonging to one of the groups.\n",
        "        '''\n",
        "        # dict_groups = self.groups[l]\n",
        "        dict_groups = self.get_groups(l=l)\n",
        "        p_tw_w = dict_groups['p_tw_w']\n",
        "        p_td_d = dict_groups['p_td_d']\n",
        "        return p_td_d,p_tw_w\n",
        "\n",
        "\n",
        "    def print_topics(self,l=0,format='csv',path_save = ''):\n",
        "        '''\n",
        "        Print topics, topic-distributions, and document clusters for a given level in the hierarchy.\n",
        "        format: csv (default) or html\n",
        "        '''\n",
        "        V=self.get_V()\n",
        "        D=self.get_D()\n",
        "\n",
        "        ## topics\n",
        "        dict_topics = self.topics(l=l,n=-1)\n",
        "\n",
        "        list_topics = sorted(list(dict_topics.keys()))\n",
        "        list_columns = ['Topic %s'%(t+1) for t in list_topics]\n",
        "\n",
        "        T = len(list_topics)\n",
        "        df = pd.DataFrame(columns = list_columns,index=range(V))\n",
        "\n",
        "\n",
        "        for t in list_topics:\n",
        "            list_w = [h[0] for h in dict_topics[t]]\n",
        "            V_t = len(list_w)\n",
        "            df.iloc[:V_t,t] = list_w\n",
        "        df=df.dropna(how='all',axis=0)\n",
        "        if format == 'csv':\n",
        "            fname_save = 'topsbm_level_%s_topics.csv'%(l)\n",
        "            filename = os.path.join(path_save,fname_save)\n",
        "            df.to_csv(filename,index=False,na_rep='')\n",
        "        elif format == 'html':\n",
        "            fname_save = 'topsbm_level_%s_topics.html'%(l)\n",
        "            filename = os.path.join(path_save,fname_save)\n",
        "            df.to_html(filename,index=False,na_rep='')\n",
        "        elif format=='tsv':\n",
        "            fname_save = 'topsbm_level_%s_topics.tsv'%(l)\n",
        "            filename = os.path.join(path_save,fname_save)\n",
        "            df.to_csv(filename,index=False,na_rep='',sep='\\t')\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        ## topic distributions\n",
        "        list_columns = ['i_doc','doc']+['Topic %s'%(t+1) for t in list_topics]\n",
        "        df = pd.DataFrame(columns=list_columns,index=range(D))\n",
        "        for i_doc in range(D):\n",
        "            list_topicdist = self.topicdist(i_doc,l=l)\n",
        "            df.iloc[i_doc,0] = i_doc\n",
        "            df.iloc[i_doc,1] = self.documents[i_doc]\n",
        "            df.iloc[i_doc,2:] = [h[1] for h in list_topicdist]\n",
        "        df=df.dropna(how='all',axis=1)\n",
        "        if format == 'csv':\n",
        "            fname_save = 'topsbm_level_%s_topic-dist.csv'%(l)\n",
        "            filename = os.path.join(path_save,fname_save)\n",
        "            df.to_csv(filename,index=False,na_rep='')\n",
        "        elif format == 'html':\n",
        "            fname_save = 'topsbm_level_%s_topic-dist.html'%(l)\n",
        "            filename = os.path.join(path_save,fname_save)\n",
        "            df.to_html(filename,index=False,na_rep='')\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        ## doc-groups\n",
        "\n",
        "        dict_clusters = self.clusters(l=l,n=-1)\n",
        "\n",
        "        list_clusters = sorted(list(dict_clusters.keys()))\n",
        "        list_columns = ['Cluster %s'%(t+1) for t in list_clusters]\n",
        "\n",
        "        T = len(list_clusters)\n",
        "        df = pd.DataFrame(columns = list_columns,index=range(D))\n",
        "\n",
        "\n",
        "        for t in list_clusters:\n",
        "            list_d = [h[0] for h in dict_clusters[t]]\n",
        "            D_t = len(list_d)\n",
        "            df.iloc[:D_t,t] = list_d\n",
        "        df=df.dropna(how='all',axis=0)\n",
        "        if format == 'csv':\n",
        "            fname_save = 'topsbm_level_%s_clusters.csv'%(l)\n",
        "            filename = os.path.join(path_save,fname_save)\n",
        "            df.to_csv(filename,index=False,na_rep='')\n",
        "        elif format == 'html':\n",
        "            fname_save = 'topsbm_level_%s_clusters.html'%(l)\n",
        "            filename = os.path.join(path_save,fname_save)\n",
        "            df.to_html(filename,index=False,na_rep='')\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    ###########\n",
        "    ########### HELPER FUNCTIONS\n",
        "    ###########\n",
        "    ## get group-topic statistics\n",
        "    def get_groups(self,l=0):\n",
        "        '''\n",
        "        extract statistics on group membership of nodes form the inferred state.\n",
        "        return dictionary\n",
        "        - B_d, int, number of doc-groups\n",
        "        - B_w, int, number of word-groups\n",
        "        - p_tw_w, array B_w x V; word-group-membership:\n",
        "             prob that word-node w belongs to word-group tw: P(tw | w)\n",
        "        - p_td_d, array B_d x D; doc-group membership:\n",
        "             prob that doc-node d belongs to doc-group td: P(td | d)\n",
        "        - p_w_tw, array V x B_w; topic distribution:\n",
        "             prob of word w given topic tw P(w | tw)\n",
        "        - p_tw_d, array B_w x d; doc-topic mixtures:\n",
        "             prob of word-group tw in doc d P(tw | d)\n",
        "        - label_map, array of size N; map from group labels to indexes in the above arrays\n",
        "        '''\n",
        "        V = self.get_V()\n",
        "        D = self.get_D()\n",
        "        N = self.get_N()\n",
        "\n",
        "        g = self.g\n",
        "        state = self.state\n",
        "        state_l = state.project_level(l).copy(overlap=True)\n",
        "\n",
        "        b = gt.contiguous_map(state_l.b)\n",
        "        label_map = {}\n",
        "        for v in g.vertices():\n",
        "            label_map[state_l.b[v]] = b[v]\n",
        "        state_l = state_l.copy(b=b)\n",
        "\n",
        "        state_l_edges = state_l.get_edge_blocks() ## labeled half-edges\n",
        "\n",
        "        counts = 'count' in self.g.ep.keys()\n",
        "\n",
        "        ## count labeled half-edges, group-memberships\n",
        "        B = state_l.get_nonempty_B()\n",
        "\n",
        "        n_wb = np.zeros((V,B))  ## number of half-edges incident on word-node w and labeled as word-group tw\n",
        "        n_db = np.zeros((D,B))  ## number of half-edges incident on document-node d and labeled as document-group td\n",
        "        n_dbw = np.zeros((D,B)) ## number of half-edges incident on document-node d and labeled as word-group td\n",
        "\n",
        "        if counts:\n",
        "            eweight = g.ep[\"count\"]\n",
        "        else:\n",
        "            eweight = g.new_ep(\"int\", 1)\n",
        "\n",
        "        ze = gt.ungroup_vector_property(state_l_edges, [0,1])\n",
        "        for v1, v2, z1, z2, w in g.get_edges([ze[0], ze[1], eweight]):\n",
        "            n_db[v1, z1] += w\n",
        "            n_dbw[v1, z2] += w\n",
        "            n_wb[v2 - D, z2] += w\n",
        "\n",
        "        p_w = np.sum(n_wb,axis=1)/float(np.sum(n_wb))\n",
        "\n",
        "        ind_d = np.where(np.sum(n_db,axis=0)>0)[0]\n",
        "        Bd = len(ind_d)\n",
        "        n_db = n_db[:,ind_d]\n",
        "\n",
        "        ind_w = np.where(np.sum(n_wb,axis=0)>0)[0]\n",
        "        Bw = len(ind_w)\n",
        "        n_wb = n_wb[:,ind_w]\n",
        "\n",
        "        ind_w2 = np.where(np.sum(n_dbw,axis=0)>0)[0]\n",
        "        n_dbw = n_dbw[:,ind_w2]\n",
        "\n",
        "        ## group-membership distributions\n",
        "        # group membership of each word-node P(t_w | w)\n",
        "        p_tw_w = (n_wb/np.sum(n_wb,axis=1)[:,np.newaxis]).T\n",
        "\n",
        "        # group membership of each doc-node P(t_d | d)\n",
        "        p_td_d = (n_db/np.sum(n_db,axis=1)[:,np.newaxis]).T\n",
        "\n",
        "        ## topic-distribution for words P(w | t_w)\n",
        "        p_w_tw = n_wb/np.sum(n_wb,axis=0)[np.newaxis,:]\n",
        "\n",
        "        ## Mixture of word-groups into documetns P(t_w | d)\n",
        "        p_tw_d = (n_dbw/np.sum(n_dbw,axis=1)[:,np.newaxis]).T\n",
        "\n",
        "\n",
        "        result = {}\n",
        "        result['Bd'] = Bd\n",
        "        result['Bw'] = Bw\n",
        "        result['p_tw_w'] = p_tw_w\n",
        "        result['p_td_d'] = p_td_d\n",
        "        result['p_w_tw'] = p_w_tw\n",
        "        result['p_tw_d'] = p_tw_d\n",
        "        result['label_map'] = label_map\n",
        "\n",
        "        return result\n",
        "\n",
        "    ### helper functions\n",
        "\n",
        "    def get_V(self):\n",
        "        '''\n",
        "        return number of word-nodes == types\n",
        "        '''\n",
        "        return int(np.sum(self.g.vp['kind'].a==1)) # no. of types\n",
        "    def get_D(self):\n",
        "        '''\n",
        "        return number of doc-nodes == number of documents\n",
        "        '''\n",
        "        return int(np.sum(self.g.vp['kind'].a==0)) # no. of types\n",
        "    def get_N(self):\n",
        "        '''\n",
        "        return number of edges == tokens\n",
        "        '''\n",
        "        return int(self.g.num_edges()) # no. of types\n",
        "\n",
        "    def group_to_group_mixture(self,l=0,norm=True):\n",
        "        V = self.get_V()\n",
        "        D = self.get_D()\n",
        "        N = self.get_N()\n",
        "\n",
        "        g = self.g\n",
        "        state = self.state\n",
        "        state_l = state.project_level(l).copy(overlap=True)\n",
        "        state_l_edges = state_l.get_edge_blocks() ## labeled half-edges\n",
        "\n",
        "        ## count labeled half-edges, group-memberships\n",
        "        B = state_l.get_B()\n",
        "        n_td_tw = np.zeros((B,B))\n",
        "\n",
        "        counts = 'count' in self.g.ep.keys()\n",
        "\n",
        "        for e in g.edges():\n",
        "            z1,z2 = state_l_edges[e]\n",
        "            if counts:\n",
        "                n_td_tw[z1 , z2] += g.ep[\"count\"][e]\n",
        "            else:\n",
        "                n_td_tw[z1, z2] += 1\n",
        "\n",
        "\n",
        "        ind_d = np.where(np.sum(n_td_tw,axis=1)>0)[0]\n",
        "        Bd = len(ind_d)\n",
        "        ind_w = np.where(np.sum(n_td_tw,axis=0)>0)[0]\n",
        "        Bw = len(ind_w)\n",
        "\n",
        "        n_td_tw = n_td_tw[:Bd,Bd:]\n",
        "        if norm == True:\n",
        "            return n_td_tw/np.sum(n_td_tw)\n",
        "        else:\n",
        "            return n_td_tw\n",
        "\n",
        "    def pmi_td_tw(self,l=0):\n",
        "        '''\n",
        "        Point-wise mutual information between topic-groups and doc-groups, S(td,tw)\n",
        "        This is an array of shape Bd x Bw.\n",
        "        It corresponds to\n",
        "        S(td,tw) = log P(tw | td) / \\tilde{P}(tw | td) .\n",
        "        This is the log-ratio between\n",
        "        P(tw | td) == prb of topic tw in doc-group td;\n",
        "        \\tilde{P}(tw | td) = P(tw) expected prob of topic tw in doc-group td under random null model.\n",
        "        '''\n",
        "        p_td_tw = self.group_to_group_mixture(l=l)\n",
        "        p_tw_td = p_td_tw.T\n",
        "        p_td = np.sum(p_tw_td,axis=0)\n",
        "        p_tw = np.sum(p_tw_td,axis=1)\n",
        "        pmi_td_tw = np.log(p_tw_td/(p_td*p_tw[:,np.newaxis])).T\n",
        "        return pmi_td_tw\n",
        "\n",
        "\n",
        "    def print_summary(self, tofile=True):\n",
        "        '''\n",
        "        Print hierarchy summary\n",
        "        '''\n",
        "        if tofile:\n",
        "            orig_stdout = sys.stdout\n",
        "            f = open('summary.txt', 'w')\n",
        "            sys.stdout = f\n",
        "            self.state.print_summary()\n",
        "            sys.stdout = orig_stdout\n",
        "            f.close()\n",
        "        else:\n",
        "            self.state.print_summary()\n",
        "\n",
        "    def plot_topic_dist(self, l):\n",
        "        groups = self.groups[l]\n",
        "        p_w_tw = groups['p_w_tw']\n",
        "        fig=plt.figure(figsize=(12,10))\n",
        "        plt.imshow(p_w_tw,origin='lower',aspect='auto',interpolation='none')\n",
        "        plt.title(r'Word group membership $P(w | tw)$')\n",
        "        plt.xlabel('Topic, tw')\n",
        "        plt.ylabel('Word w (index)')\n",
        "        plt.colorbar()\n",
        "        fig.savefig(\"p_w_tw_%d.png\"%l)\n",
        "        p_tw_d = groups['p_tw_d']\n",
        "        fig=plt.figure(figsize=(12,10))\n",
        "        plt.imshow(p_tw_d,origin='lower',aspect='auto',interpolation='none')\n",
        "        plt.title(r'Word group membership $P(tw | d)$')\n",
        "        plt.xlabel('Document (index)')\n",
        "        plt.ylabel('Topic, tw')\n",
        "        plt.colorbar()\n",
        "        fig.savefig(\"p_tw_d_%d.png\"%l)\n",
        "\n",
        "    def save_data(self):\n",
        "        for i in range(len(self.state.get_levels())-2)[::-1]:\n",
        "            print(\"Saving level %d\"%i)\n",
        "            self.print_topics(l=i)\n",
        "            self.print_topics(l=i, format='tsv')\n",
        "            self.plot_topic_dist(i)\n",
        "            e = self.state.get_levels()[i].get_matrix()\n",
        "            plt.matshow(e.todense())\n",
        "            plt.savefig(\"mat_%d.png\"%i)\n",
        "        self.print_summary()"
      ],
      "metadata": {
        "id": "eZK6k_DJeA_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import pylab as plt\n",
        "%matplotlib inline  \n"
      ],
      "metadata": {
        "id": "AHLzJjyId9Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_data = ''\n",
        "\n",
        "## texts\n",
        "fname_data = '/content/hSBM_Topicmodel/corpus.txt'\n",
        "filename = os.path.join(path_data,fname_data)\n",
        "\n",
        "with open(filename,'r', encoding = 'utf8') as f:\n",
        "    x = f.readlines()\n",
        "texts = [h.split() for h in x]\n",
        "\n",
        "## titles\n",
        "fname_data = '/content/hSBM_Topicmodel/titles.txt'\n",
        "filename = os.path.join(path_data,fname_data)\n",
        "\n",
        "with open(filename,'r', encoding = 'utf8') as f:\n",
        "    x = f.readlines()\n",
        "titles = [h.split()[0] for h in x]"
      ],
      "metadata": {
        "id": "30kkYLihdhiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i_doc = 0\n",
        "print(titles[0])\n",
        "print(texts[i_doc][:10])"
      ],
      "metadata": {
        "id": "WBkTa0wAd72m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## we create an instance of the sbmtm-class\n",
        "model = sbmtm()\n",
        "\n",
        "## we have to create the word-document network from the corpus\n",
        "model.make_graph(texts,documents=titles)\n",
        "\n",
        "## we can also skip the previous step by saving/loading a graph\n",
        "# model.save_graph(filename = 'graph.xml.gz')\n",
        "# model.load_graph(filename = 'graph.xml.gz')\n",
        "\n",
        "## fit the model\n",
        "gt.seed_rng(32) ## seed for graph-tool's random number generator --> same results\n",
        "model.fit()"
      ],
      "metadata": {
        "id": "qimmockUfbSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "ntpKLF4XffGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "#Sentences we want to encode. Example:\n",
        "sentence = ['This framework generates embeddings for each input sentence. This is second sentence.']\n",
        "\n",
        "\n",
        "#Sentences are encoded by calling model.encode()\n",
        "embedding = model.encode(sentence)"
      ],
      "metadata": {
        "id": "bWLFHtiogsb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding.shape"
      ],
      "metadata": {
        "id": "ioaUbJu6g3Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TransfoXLTokenizer, TransfoXLModel\n",
        "import torch\n",
        "\n",
        "tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')\n",
        "model = TransfoXLModel.from_pretrained('transfo-xl-wt103')\n",
        "\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple"
      ],
      "metadata": {
        "id": "dFZHfv9XhDRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/l294265421/OTE-MTL-ASOTE"
      ],
      "metadata": {
        "id": "MKMn_tHyiN6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textattack"
      ],
      "metadata": {
        "id": "h6j66C3xkYsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "id": "KF6mxmU-3X6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textattack.augmentation import WordNetAugmenter, EmbeddingAugmenter, CharSwapAugmenter, CheckListAugmenter"
      ],
      "metadata": {
        "id": "M1My7WgB3yop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug = CheckListAugmenter(pct_words_to_swap=0.2, transformations_per_example=5)\n",
        "#Name Replacement, Location Replacement, Number Alteration, and Contraction/Extension. \n",
        "sample = \"I'd love to go to Japan but the tickets are 500 dollars\"\n",
        "aug.augment(sample)"
      ],
      "metadata": {
        "id": "0xFtF_Hm3dXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Leadership requires two things: a vision of the world that does not yet exist and the ability to communicate it.\""
      ],
      "metadata": {
        "id": "oPhBb54I3hmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug = WordNetAugmenter(pct_words_to_swap=0.4, transformations_per_example=5)\n",
        "#\n",
        "aug.augment(sample)"
      ],
      "metadata": {
        "id": "avsp4oZp3mDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug = CharSwapAugmenter()\n",
        "\n",
        "aug.augment(text)"
      ],
      "metadata": {
        "id": "_o65qyq03pjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jbuoxg7L4eH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "mzAWUOFg4bcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/SentimentalAnalysisModel/IMDB Dataset.csv')"
      ],
      "metadata": {
        "id": "j6t0YkJa4VkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_aug = df.copy()\n",
        "from textattack.augmentation import EmbeddingAugmenter\n",
        "aug = EmbeddingAugmenter()\n",
        "\n",
        "train_aug['review'] = train_aug['review'].apply(lambda x: str(aug.augment(x)))\n",
        "\n",
        "df = df.append(train_aug, ignore_index=True)"
      ],
      "metadata": {
        "id": "FHKo_bv13t85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k_o2LpRQ36Z1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}